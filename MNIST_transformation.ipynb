{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pSvOo5AIqtNX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm_notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Setting the seed for Python's built-in random module\n",
        "random.seed(42)\n",
        "\n",
        "# Setting the seed for numpy's random number generator\n",
        "np.random.seed(42)\n",
        "\n",
        "# Setting the seed for PyTorch (both CPU and GPU)\n",
        "torch.manual_seed(2147483647)\n",
        "\n",
        "# If using CUDA, set the seed for all GPUs\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(2147483647)\n",
        "\n",
        "# Ensuring deterministic behavior in PyTorch\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# Optionally, set the environment variable for deterministic behavior in cuDNN\n",
        "import os\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Select the device\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmCNjIkArE_a",
        "outputId": "f35bf45d-4c97-4bd4-ff92-f45c4b49fc4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
        "])\n",
        "\n",
        "# Download and prepare the MNIST dataset\n",
        "mnist_train_val = torchvision.datasets.MNIST(\n",
        "    root='datasets/mnist', train=True, transform=transform, download=True)\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    root='datasets/mnist', train=False, transform=transform, download=True)\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = 50000\n",
        "val_size = len(mnist_train_val) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(mnist_train_val, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=32, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=32, shuffle=False)\n",
        "\n",
        "# Print the sizes of the datasets to verify\n",
        "print(f'Training set size: {len(train_subset)}')\n",
        "print(f'Validation set size: {len(val_subset)}')\n",
        "print(f'Test set size: {len(mnist_test)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPJ8IRJ7r9CM",
        "outputId": "604c98d5-96dc-4e15-db07-feeaaffdd7ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno -5] No address associated with hostname>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to datasets/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 130192212.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/mnist/MNIST/raw/train-images-idx3-ubyte.gz to datasets/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to datasets/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 49182173.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to datasets/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to datasets/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 86355809.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to datasets/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to datasets/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4751940.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to datasets/mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 50000\n",
            "Validation set size: 10000\n",
            "Test set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# Function to print dimensions of images and labels in the data loader\n",
        "def check_loader_dimensions(loaders):\n",
        "    for name, loader in loaders.items():\n",
        "        num_samples = len(loader.dataset)\n",
        "        data_iter = iter(loader)\n",
        "        images, labels = next(data_iter)\n",
        "        print(f'Loader: {name}')\n",
        "        print(f'Number of samples: {num_samples}')\n",
        "        print(f'Image batch shape: {images.shape}')\n",
        "        print(f'Label batch shape: {labels.shape}')\n",
        "        print('------------------------')\n",
        "\n",
        "# Define preprocessing transformations\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download and prepare the MNIST dataset\n",
        "mnist_train_val = torchvision.datasets.MNIST(\n",
        "    root='datasets/mnist', train=True, transform=transform, download=True)\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    root='datasets/mnist', train=False, transform=transform, download=True)\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = 50000\n",
        "val_size = len(mnist_train_val) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(mnist_train_val, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "loaders = {\n",
        "    'train_loader': torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True),\n",
        "    'val_loader': torch.utils.data.DataLoader(val_subset, batch_size=32, shuffle=False),\n",
        "    'test_loader': torch.utils.data.DataLoader(mnist_test, batch_size=32, shuffle=False)\n",
        "}\n",
        "\n",
        "# Check and print dimensions\n",
        "check_loader_dimensions(loaders)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnlreVR3xjpk",
        "outputId": "395ebdb8-64d4-464a-bfa8-9413f8e7e347"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader: train_loader\n",
            "Number of samples: 50000\n",
            "Image batch shape: torch.Size([32, 1, 28, 28])\n",
            "Label batch shape: torch.Size([32])\n",
            "------------------------\n",
            "Loader: val_loader\n",
            "Number of samples: 10000\n",
            "Image batch shape: torch.Size([32, 1, 28, 28])\n",
            "Label batch shape: torch.Size([32])\n",
            "------------------------\n",
            "Loader: test_loader\n",
            "Number of samples: 10000\n",
            "Image batch shape: torch.Size([32, 1, 28, 28])\n",
            "Label batch shape: torch.Size([32])\n",
            "------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define preprocessing transformations\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download and prepare the MNIST dataset\n",
        "mnist_train_val = torchvision.datasets.MNIST(\n",
        "    root='datasets/mnist', train=True, transform=transform, download=True)\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    root='datasets/mnist', train=False, transform=transform, download=True)\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = 50000\n",
        "val_size = len(mnist_train_val) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(mnist_train_val, [train_size, val_size])\n",
        "\n",
        "# Visualize examples\n",
        "def visualize_examples(dataset, num_images=4):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    for i in range(num_images):\n",
        "        image, label = dataset[i]\n",
        "        image = image.squeeze().numpy()\n",
        "\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.title(f'Label: {label}')\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(\"Image examples of the MNIST dataset\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize examples from the validation set\n",
        "visualize_examples(val_subset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "xcNOz4uxx9zg",
        "outputId": "1f41fbdc-118f-4af8-adbd-1a7c7f299496"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model hyperparameters and configurations using a dictionary\n",
        "model_config = {\n",
        "    'image_size': 28,\n",
        "    'embed_dim': 256,\n",
        "    'hidden_dim': 256 * 3,\n",
        "    'num_heads': 8,\n",
        "    'num_layers': 6,\n",
        "    'patch_size': 7,\n",
        "    'num_patches': 16,\n",
        "    'num_channels': 1,\n",
        "    'num_classes': 10,\n",
        "    'dropout': 0.2\n",
        "}\n",
        "\n",
        "# Accessing the parameters from the dictionary\n",
        "image_size = model_config['image_size']\n",
        "embed_dim = model_config['embed_dim']\n",
        "hidden_dim = model_config['hidden_dim']\n",
        "num_heads = model_config['num_heads']\n",
        "num_layers = model_config['num_layers']\n",
        "patch_size = model_config['patch_size']\n",
        "num_patches = model_config['num_patches']\n",
        "num_channels = model_config['num_channels']\n",
        "num_classes = model_config['num_classes']\n",
        "dropout = model_config['dropout']\n",
        "\n",
        "# Print the configurations to verify\n",
        "for key, value in model_config.items():\n",
        "    print(f'{key}: {value}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc-HsSTFyh64",
        "outputId": "2f6026ea-91be-4b09-abf5-24386c006a30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_size: 28\n",
            "embed_dim: 256\n",
            "hidden_dim: 768\n",
            "num_heads: 8\n",
            "num_layers: 6\n",
            "patch_size: 7\n",
            "num_patches: 16\n",
            "num_channels: 1\n",
            "num_classes: 10\n",
            "dropout: 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_patches(images, patch_size, flatten_channels=True):\n",
        "\n",
        "    # Get the shape of the input images\n",
        "    batch_size, num_channels, height, width = images.shape  # MNIST: [B, 1, 28, 28]\n",
        "\n",
        "    # Calculate the number of patches along height and width\n",
        "    num_patches_height = height // patch_size\n",
        "    num_patches_width = width // patch_size\n",
        "\n",
        "    # Reshape to separate patches\n",
        "    reshaped_images = images.reshape(batch_size, num_channels, num_patches_height, patch_size, num_patches_width, patch_size)  # [B, C, H//p, p, W//p, p]\n",
        "\n",
        "    # Permute dimensions to bring patches to the front\n",
        "    permuted_images = reshaped_images.permute(0, 2, 4, 1, 3, 5)  # [B, H//p, W//p, C, p, p]\n",
        "\n",
        "    # Flatten the height and width dimensions to get patches\n",
        "    patches = permuted_images.flatten(1, 2)  # [B, (H//p)*(W//p), C, p, p]\n",
        "\n",
        "    if flatten_channels:\n",
        "        # Flatten the channel and patch dimensions to get a vector representation of patches\n",
        "        patches = patches.flatten(2, 4)  # [B, (H//p)*(W//p), C*p*p]\n",
        "\n",
        "    return patches\n",
        "\n",
        "\n",
        "x = torch.randn(32, 1, 28, 28)\n",
        "patches = image_to_patches(x, patch_size=7, flatten_channels=True)\n",
        "print(patches.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K7NLmPTyybQ",
        "outputId": "b7883bd6-04ae-4db7-833a-dd1aa32413cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 16, 49])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define preprocessing transformations\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download and prepare the MNIST dataset\n",
        "mnist_train_val = torchvision.datasets.MNIST(\n",
        "    root='datasets/mnist', train=True, transform=transform, download=True)\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    root='datasets/mnist', train=False, transform=transform, download=True)\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = 50000\n",
        "val_size = len(mnist_train_val) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(mnist_train_val, [train_size, val_size])\n",
        "\n",
        "# Function to convert images to patches\n",
        "def image_to_patches(images, patch_size, flatten_channels=True):\n",
        "\n",
        "    batch_size, num_channels, height, width = images.shape  # MNIST: [B, 1, 28, 28]\n",
        "\n",
        "    # Calculate the number of patches along height and width\n",
        "    num_patches_height = height // patch_size\n",
        "    num_patches_width = width // patch_size\n",
        "\n",
        "    # Reshape to separate patches\n",
        "    reshaped_images = images.reshape(batch_size, num_channels, num_patches_height, patch_size, num_patches_width, patch_size)  # [B, C, H//p, p, W//p, p]\n",
        "\n",
        "    # Permute dimensions to bring patches to the front\n",
        "    permuted_images = reshaped_images.permute(0, 2, 4, 1, 3, 5)  # [B, H//p, W//p, C, p, p]\n",
        "\n",
        "    # Flatten the height and width dimensions to get patches\n",
        "    patches = permuted_images.flatten(1, 2)  # [B, (H//p)*(W//p), C, p, p]\n",
        "\n",
        "    if flatten_channels:\n",
        "        # Flatten the channel and patch dimensions to get a vector representation of patches\n",
        "        patches = patches.flatten(2, 4)  # [B, (H//p)*(W//p), C*p*p]\n",
        "\n",
        "    return patches\n",
        "\n",
        "# Function to visualize image patches\n",
        "def visualize_image_patches(images, patch_size, num_examples=4):\n",
        "\n",
        "    # Get the image patches\n",
        "    img_patches = image_to_patches(images[:num_examples], patch_size, flatten_channels=False)\n",
        "\n",
        "    # Create the subplot\n",
        "    fig, axes = plt.subplots(num_examples, 1, figsize=(14, 12))\n",
        "    fig.suptitle(\"Images as input sequences of patches\")\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        # Create a grid of patches\n",
        "        img_grid = torchvision.utils.make_grid(img_patches[i], nrow=int(images.shape[2] / patch_size), normalize=True, pad_value=0.9)\n",
        "        img_grid = img_grid.permute(1, 2, 0)  # Change shape for plotting\n",
        "\n",
        "        # Plot the grid of patches\n",
        "        axes[i].imshow(img_grid)\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Visualize some examples from the validation set\n",
        "visualize_image_patches(torch.stack([val_subset[idx][0] for idx in range(4)], dim=0), patch_size=7)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B1XPspmjy3V_",
        "outputId": "e2d5af17-5daa-486c-b583-912bca662e83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1200 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define preprocessing transformations\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download and prepare the MNIST dataset\n",
        "mnist_train_val = torchvision.datasets.MNIST(\n",
        "    root='datasets/mnist', train=True, transform=transform, download=True)\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    root='datasets/mnist', train=False, transform=transform, download=True)\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = 50000\n",
        "val_size = len(mnist_train_val) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(mnist_train_val, [train_size, val_size])\n",
        "\n",
        "class ImagePatcher:\n",
        "    def __init__(self, patch_size):\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def to_patches(self, images, flatten_channels=True):\n",
        "\n",
        "        batch_size, num_channels, height, width = images.shape  # MNIST: [B, 1, 28, 28]\n",
        "\n",
        "        # Calculate the number of patches along height and width\n",
        "        num_patches_height = height // self.patch_size\n",
        "        num_patches_width = width // self.patch_size\n",
        "\n",
        "        # Reshape to separate patches\n",
        "        reshaped_images = images.reshape(batch_size, num_channels, num_patches_height, self.patch_size, num_patches_width, self.patch_size)  # [B, C, H//p, p, W//p, p]\n",
        "\n",
        "        # Permute dimensions to bring patches to the front\n",
        "        permuted_images = reshaped_images.permute(0, 2, 4, 1, 3, 5)  # [B, H//p, W//p, C, p, p]\n",
        "\n",
        "        # Flatten the height and width dimensions to get patches\n",
        "        patches = permuted_images.flatten(1, 2)  # [B, (H//p)*(W//p), C, p, p]\n",
        "\n",
        "        if flatten_channels:\n",
        "            # Flatten the channel and patch dimensions to get a vector representation of patches\n",
        "            patches = patches.flatten(2, 4)  # [B, (H//p)*(W//p), C*p*p]\n",
        "\n",
        "        return patches\n",
        "\n",
        "\n",
        "class PatchVisualizer:\n",
        "    def __init__(self, patch_size):\n",
        "        self.patcher = ImagePatcher(patch_size)\n",
        "\n",
        "    def visualize(self, images, num_examples=4):\n",
        "\n",
        "        # Get the image patches\n",
        "        img_patches = self.patcher.to_patches(images[:num_examples], flatten_channels=False)\n",
        "\n",
        "        # Create the subplot\n",
        "        fig, axes = plt.subplots(num_examples, 1, figsize=(14, 12))\n",
        "        fig.suptitle(\"Images as input sequences of patches\")\n",
        "\n",
        "        for i in range(num_examples):\n",
        "            # Create a grid of patches\n",
        "            img_grid = torchvision.utils.make_grid(img_patches[i], nrow=int(images.shape[2] / self.patcher.patch_size), normalize=True, pad_value=0.9)\n",
        "            img_grid = img_grid.permute(1, 2, 0)  # Change shape for plotting\n",
        "\n",
        "            # Plot the grid of patches\n",
        "            axes[i].imshow(img_grid)\n",
        "            axes[i].axis(\"off\")\n",
        "\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "# Instantiate the visualizer and visualize some examples from the validation set\n",
        "visualizer = PatchVisualizer(patch_size=7)\n",
        "visualizer.visualize(torch.stack([val_subset[idx][0] for idx in range(4)], dim=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tn4aTzDqzUhK",
        "outputId": "7472ac90-d388-4066-96c8-1180d553ca56"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1200 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim=None, dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        # Linear transformations for queries, keys, and values\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim if hidden_dim else embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim if hidden_dim else embed_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.size()  # Batch size, sequence length, embed dim\n",
        "\n",
        "        # Linear transformations\n",
        "        queries = self.query(x)\n",
        "        keys = self.key(x)\n",
        "        values = self.value(x)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        queries = queries.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)  # B x num_heads x N x head_dim\n",
        "        keys = keys.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)  # B x num_heads x N x head_dim\n",
        "        values = values.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)  # B x num_heads x N x head_dim\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) * self.scale  # B x num_heads x N x N\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, values)  # B x num_heads x N x head_dim\n",
        "\n",
        "        # Combine heads and reshape\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(B, N, C)  # B x N x C\n",
        "\n",
        "        # Add and norm\n",
        "        x = x + attention_output\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Feed-forward network\n",
        "        x = x + self.linear(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zYEOQP3Lz1UO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, C, H, W) -> (B, embed_dim, H_out, W_out)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, embed_dim, H_out * W_out)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.size()\n",
        "        queries = self.query(x)\n",
        "        keys = self.key(x)\n",
        "        values = self.value(x)\n",
        "\n",
        "        queries = queries.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        keys = keys.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        values = values.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) * self.scale\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        attention_output = torch.matmul(self.dropout(attention_weights), values)\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(B, N, C)\n",
        "\n",
        "        return self.out(attention_output)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        num_channels,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        num_classes,\n",
        "        dropout=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(image_size, patch_size, num_channels, embed_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        self.transformer = nn.ModuleList([\n",
        "            Attention(embed_dim, num_heads, dropout=dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            MLP(embed_dim, embed_dim * 4, dropout=dropout),\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x = x + self.pos_embedding\n",
        "\n",
        "        for layer in self.transformer:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.mlp_head(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "s0SFSpDL2o4R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        hidden_dim,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        patch_size,\n",
        "        num_channels,\n",
        "        num_patches,\n",
        "        num_classes,\n",
        "        dropout=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(patch_size, num_channels, embed_dim, image_size=224)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        self.transformer = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x = x + self.pos_embedding\n",
        "\n",
        "        for block in self.transformer:\n",
        "            x = block(x)\n",
        "\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.mlp_head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size, num_channels, embed_dim, image_size=224):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.image_size = image_size\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.attention = Attention(embed_dim, num_heads, dropout)\n",
        "        self.mlp = MLP(embed_dim, hidden_dim, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(x)\n",
        "        x = x + self.mlp(x)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.size()\n",
        "        queries = self.query(x)\n",
        "        keys = self.key(x)\n",
        "        values = self.value(x)\n",
        "\n",
        "        queries = queries.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        keys = keys.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        values = values.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) * self.scale\n",
        "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "        attention_output = torch.matmul(self.dropout(attention_weights), values)\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(B, N, C)\n",
        "\n",
        "        return self.out(attention_output)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Usage\n",
        "embed_dim = 256\n",
        "hidden_dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "patch_size = 16\n",
        "num_channels = 3\n",
        "num_patches = (224 // patch_size) ** 2\n",
        "num_classes = 10\n",
        "dropout = 0.1\n",
        "\n",
        "# Instantiate the model\n",
        "model = VisionTransformer(\n",
        "    embed_dim=embed_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    patch_size=patch_size,\n",
        "    num_channels=num_channels,\n",
        "    num_patches=num_patches,\n",
        "    num_classes=num_classes,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "# Transfer to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Model restoration logic\n",
        "model_restore = '/path/to/your/model.pth'\n",
        "if model_restore and os.path.isfile(model_restore):\n",
        "    model.load_state_dict(torch.load(model_restore, map_location=device))\n",
        "    model.restored = True\n",
        "    print(\"Model restored from\", model_restore)\n",
        "else:\n",
        "    print(\"No checkpoint found at\", model_restore)\n",
        "\n",
        "# Example usage\n",
        "input_data = torch.randn(1, 3, 224, 224).to(device)\n",
        "output = model(input_data)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shBlYj8W3Fdo",
        "outputId": "978487bf-826f-4452-b4b0-f0e1f90bcfac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found at /path/to/your/model.pth\n",
            "tensor([[ 0.6042, -0.3989, -0.6561, -0.0827,  0.8026, -0.0070, -1.0003,  0.4979,\n",
            "          0.9631, -0.0143]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCvPpAG33i2P",
        "outputId": "8289cdc8-0aa9-4a73-e435-a222e4a90992"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbedding(\n",
              "    (proj): Conv2d(3, 256, kernel_size=(16, 16), stride=(16, 16))\n",
              "  )\n",
              "  (transformer): ModuleList(\n",
              "    (0-5): 6 x TransformerBlock(\n",
              "      (attention): Attention(\n",
              "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlp_head): Sequential(\n",
              "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming VisionTransformer and other components are already defined\n",
        "# Define VisionTransformer and other necessary components (already provided earlier)\n",
        "\n",
        "# Function to evaluate model accuracy on test set\n",
        "def evaluate_model_accuracy(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(test_loader, desc='Evaluating'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            preds = model(imgs)\n",
        "            _, pred_cls = preds.max(1)\n",
        "            correct_predictions += pred_cls.eq(labels).sum().item()\n",
        "\n",
        "    accuracy = correct_predictions / len(test_loader.dataset)\n",
        "    return accuracy\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define hyperparameters and other settings\n",
        "    embed_dim = 256\n",
        "    hidden_dim = 512\n",
        "    num_heads = 8\n",
        "    num_layers = 6\n",
        "    patch_size = 7  # Adjusted for MNIST (28x28 images with 4x4 patches)\n",
        "    num_channels = 1  # MNIST is grayscale\n",
        "    num_patches = (28 // patch_size) ** 2\n",
        "    num_classes = 10  # MNIST has 10 classes\n",
        "    dropout = 0.1\n",
        "    batch_size = 64\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = VisionTransformer(\n",
        "        embed_dim=embed_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "        patch_size=patch_size,\n",
        "        num_channels=num_channels,\n",
        "        num_patches=num_patches,\n",
        "        num_classes=num_classes,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    # Load pre-trained weights if available\n",
        "    # model.load_state_dict(torch.load('path_to_your_saved_model.pth'))\n",
        "\n",
        "    # Load test dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Evaluate model accuracy on test set\n",
        "    test_accuracy = evaluate_model_accuracy(model, test_loader, device)\n",
        "    print(f'Accuracy on test set: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W4ICYMLZre3",
        "outputId": "45f1dc60-8941-4329-e499-fbeead79ea96"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 120796067.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 54126762.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 32079799.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8581319.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:33<00:00,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 9.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the MNIST test dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Find the index of the 5th occurrence of label 8 in the test dataset\n",
        "label_to_find = 8\n",
        "count = 0\n",
        "target_index = None\n",
        "\n",
        "for i in range(len(test_dataset.targets)):\n",
        "    if test_dataset.targets[i] == label_to_find:\n",
        "        count += 1\n",
        "        if count == 5:  # Adjust this number to find the Nth occurrence\n",
        "            target_index = i\n",
        "            break\n",
        "\n",
        "if target_index is not None:\n",
        "    # Perform any operation using the found index\n",
        "    print(f\"Index of the 5th sample with label {label_to_find}: {target_index}\")\n",
        "    # Example: Access the image and label\n",
        "    image, label = test_dataset[target_index]\n",
        "    print(f\"Label: {label}\")\n",
        "    # Perform further operations with the image or label\n",
        "else:\n",
        "    print(f\"No sample found with label {label_to_find}\")\n",
        "\n",
        "# Example usage of evaluating model accuracy on test set\n",
        "def evaluate_model_accuracy(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(test_loader, desc='Evaluating'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            preds = model(imgs)\n",
        "            _, pred_cls = preds.max(1)\n",
        "            correct_predictions += pred_cls.eq(labels).sum().item()\n",
        "\n",
        "    accuracy = correct_predictions / len(test_loader.dataset)\n",
        "    return accuracy\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming VisionTransformer and other components are defined earlier\n",
        "    # Initialize hyperparameters, model, device, etc.\n",
        "\n",
        "    # Load pre-trained weights if available\n",
        "    # model.load_state_dict(torch.load('path_to_your_saved_model.pth'))\n",
        "\n",
        "    # Load test dataset (already loaded as test_dataset above)\n",
        "\n",
        "    # Example of evaluating model accuracy on the entire test set\n",
        "    batch_size = 64\n",
        "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Evaluate model accuracy on test set\n",
        "    test_accuracy = evaluate_model_accuracy(model, test_loader, device)\n",
        "    print(f'Accuracy on test set: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGj88E-qbegg",
        "outputId": "5edafc79-d91d-4399-f578-022bd341a1d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of the 5th sample with label 8: 134\n",
            "Label: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:25<00:00,  6.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 9.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define VisionTransformer class\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, num_heads, num_layers, patch_size, num_channels, num_patches, num_classes):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.input_layer = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "        self.transformer = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.input_layer(x)  # Shape: [batch_size, embed_dim, num_patches**0.5, num_patches**0.5]\n",
        "        patches = patches.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]\n",
        "        batch_size = patches.size(0)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, embed_dim]\n",
        "        x = torch.cat((cls_tokens, patches), dim=1)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
        "        x += self.pos_embedding[:, :(x.size(1)), :]  # Add position embedding\n",
        "\n",
        "        for layer in self.transformer:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.fc(x[:, 0])  # Take the cls_token output\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define hyperparameters and other settings\n",
        "    embed_dim = 256\n",
        "    hidden_dim = 256\n",
        "    num_heads = 8\n",
        "    num_layers = 6\n",
        "    patch_size = 7  # Adjust based on your model's patch size\n",
        "    num_channels = 1  # MNIST is grayscale\n",
        "    num_patches = (28 // patch_size) ** 2  # Assuming MNIST 28x28 images\n",
        "    num_classes = 10  # MNIST has 10 classes\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = VisionTransformer(\n",
        "        embed_dim=embed_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "        patch_size=patch_size,\n",
        "        num_channels=num_channels,\n",
        "        num_patches=num_patches,\n",
        "        num_classes=num_classes\n",
        "    ).to(device)\n",
        "\n",
        "    # Load test dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Select a specific test sample index where the label is 8\n",
        "    test_sample_index = (test_dataset.targets == 8).nonzero(as_tuple=True)[0][5].item()\n",
        "    img_tensor = test_dataset.data[test_sample_index].unsqueeze(0).unsqueeze(0).float()  # Shape: [1, 1, 28, 28]\n",
        "\n",
        "    # Move input tensor to device\n",
        "    img_tensor = img_tensor.to(device)\n",
        "\n",
        "    # Convert the test sample into patches using the input layer\n",
        "    patches = model.input_layer(img_tensor)  # Shape: [1, embed_dim, H, W]\n",
        "    patches = patches.view(1, embed_dim, -1).permute(0, 2, 1)  # Shape: [1, num_patches, embed_dim]\n",
        "\n",
        "    # Attach the class token and add the position embedding\n",
        "    cls_token = model.cls_token.expand(1, -1, -1)  # Shape: [1, 1, embed_dim]\n",
        "    pos_embedding = model.pos_embedding[:, :patches.size(1) + 1, :]  # Shape: [1, num_patches + 1, embed_dim]\n",
        "\n",
        "    # Concatenate class token and patches\n",
        "    transformer_input = torch.cat((cls_token, patches), dim=1) + pos_embedding\n",
        "\n",
        "    print(\"Input tensor to Transformer: \", transformer_input.shape)\n",
        "\n",
        "    # Forward pass through the transformer layers\n",
        "    for layer in model.transformer:\n",
        "        transformer_input = layer(transformer_input)\n",
        "\n",
        "    print(\"Output shape after transformer layers: \", transformer_input.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_10Mm14deNj",
        "outputId": "9cf03d49-8310-4727-90f5-098f57b48a19"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor to Transformer:  torch.Size([1, 17, 256])\n",
            "Output shape after transformer layers:  torch.Size([1, 17, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define VisionTransformer class\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, num_heads, num_layers, patch_size, num_channels, num_patches, num_classes):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.input_layer = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "        self.transformer = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.input_layer(x)  # Shape: [batch_size, embed_dim, num_patches**0.5, num_patches**0.5]\n",
        "        patches = patches.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]\n",
        "        batch_size = patches.size(0)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, embed_dim]\n",
        "        x = torch.cat((cls_tokens, patches), dim=1)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
        "        x += self.pos_embedding[:, :(x.size(1)), :]  # Add position embedding\n",
        "\n",
        "        for layer in self.transformer:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.fc(x[:, 0])  # Take the cls_token output\n",
        "        return x\n",
        "\n",
        "# Define hyperparameters and other settings\n",
        "embed_dim = 2304  # Adjusted to be divisible by 24 (3 * num_heads)\n",
        "hidden_dim = 2304\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "patch_size = 7  # Adjust based on your model's patch size\n",
        "num_channels = 1  # MNIST is grayscale\n",
        "num_patches = (28 // patch_size) ** 2  # Assuming MNIST 28x28 images\n",
        "num_classes = 10  # MNIST has 10 classes\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = VisionTransformer(\n",
        "    embed_dim=embed_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    patch_size=patch_size,\n",
        "    num_channels=num_channels,\n",
        "    num_patches=num_patches,\n",
        "    num_classes=num_classes\n",
        ").to(device)\n",
        "\n",
        "# Load test dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Select a specific test sample index where the label is 8\n",
        "test_sample_index = (test_dataset.targets == 8).nonzero(as_tuple=True)[0][5].item()\n",
        "img_tensor = test_dataset.data[test_sample_index].unsqueeze(0).unsqueeze(0).float()  # Shape: [1, 1, 28, 28]\n",
        "\n",
        "# Move input tensor to device\n",
        "img_tensor = img_tensor.to(device)\n",
        "\n",
        "# Convert the test sample into patches using the input layer\n",
        "patches = model.input_layer(img_tensor)  # Shape: [1, embed_dim, H, W]\n",
        "patches = patches.view(1, embed_dim, -1).permute(0, 2, 1)  # Shape: [1, num_patches, embed_dim]\n",
        "\n",
        "# Attach the class token and add the position embedding\n",
        "cls_token = model.cls_token.expand(1, -1, -1)  # Shape: [1, 1, embed_dim]\n",
        "pos_embedding = model.pos_embedding[:, :patches.size(1) + 1, :]  # Shape: [1, num_patches + 1, embed_dim]\n",
        "transformer_input = torch.cat((cls_token, patches), dim=1) + pos_embedding\n",
        "\n",
        "print(\"Input tensor to Transformer: \", transformer_input.shape)\n",
        "\n",
        "# Forward pass through the transformer layers\n",
        "for layer in model.transformer:\n",
        "    transformer_input = layer(transformer_input)\n",
        "\n",
        "print(\"Output shape after transformer layers: \", transformer_input.shape)\n",
        "\n",
        "# Expand the input using the first transformer layer's linear transformation\n",
        "transformer_input_expanded = model.transformer[0].linear1(transformer_input)  # Assuming linear1 exists\n",
        "\n",
        "print(\"transformer_input_expanded shape: \", transformer_input_expanded.shape)\n",
        "\n",
        "# Compute the last dimension\n",
        "last_dim = transformer_input_expanded.shape[2] // num_heads\n",
        "\n",
        "# Ensure last_dim is divisible by num_heads\n",
        "assert transformer_input_expanded.shape[2] % num_heads == 0, f\"Dimension mismatch: {transformer_input_expanded.shape[2]} is not divisible by {num_heads}\"\n",
        "\n",
        "# Split qkv into multiple q, k, and v vectors for multi-head attention\n",
        "qkv = transformer_input_expanded.view(transformer_input_expanded.size(0), transformer_input_expanded.size(1), num_heads, last_dim)\n",
        "qkv = qkv.permute(2, 0, 3, 1)  # Shape: [num_heads, batch_size, last_dim, num_patches+1]\n",
        "\n",
        "# Extract q, k, and v\n",
        "q, k, v = qkv[0], qkv[1], qkv[2]  # Each has shape: [batch_size, last_dim, num_patches+1]\n",
        "\n",
        "attention_matrix = torch.matmul(q.transpose(1, 2), k)\n",
        "\n",
        "print(\"Attention matrix shape: \", attention_matrix.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG2nrRcbLNNw",
        "outputId": "0cd9c82a-1f3a-460e-c32a-2480889f3c58"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor to Transformer:  torch.Size([1, 17, 2304])\n",
            "Output shape after transformer layers:  torch.Size([1, 17, 2304])\n",
            "transformer_input_expanded shape:  torch.Size([1, 17, 2048])\n",
            "Attention matrix shape:  torch.Size([1, 17, 17])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "attention_matrix_mean = torch.mean(attention_matrix, dim=0)\n",
        "residual_att = torch.eye(attention_matrix_mean.size(0)).to(device)\n",
        "aug_att_mat = attention_matrix_mean + residual_att\n",
        "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1, keepdim=True)\n",
        "\n",
        "joint_attentions = torch.zeros(aug_att_mat.size()).to(device)\n",
        "joint_attentions[0] = aug_att_mat[0]\n",
        "\n",
        "for n in range(1, aug_att_mat.size(0)):\n",
        "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
        "\n",
        "print(\"Final joint attentions shape:\", joint_attentions.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29wxlysMN7xJ",
        "outputId": "d75caddd-6616-46fb-ad5c-fc6a12b72086"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final joint attentions shape: torch.Size([17, 17])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "attn_heatmap = joint_attentions[0, 1:].reshape((int(image_size/patch_size), int(image_size/patch_size)))\n",
        "attn_heatmap_resized = F.interpolate(attn_heatmap.unsqueeze(0).unsqueeze(0), size=(image_size, image_size), mode='bilinear').view(image_size, image_size, 1)\n",
        "attn_heatmap_resized_np = attn_heatmap_resized.detach().cpu().numpy()\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "img = np.asarray(img_tensor.cpu().squeeze())\n",
        "ax1.imshow(img, cmap='gray')\n",
        "ax1.set_title('MNIST test sample')\n",
        "ax1.axis('off')\n",
        "ax2.imshow(attn_heatmap_resized_np.squeeze(), cmap='hot', interpolation='nearest')\n",
        "ax2.set_title('Attention heatmap')\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "WKRe1tfNOV6Y",
        "outputId": "a6057c1c-08b4-40ff-f89b-3bbe73ce3ea5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}